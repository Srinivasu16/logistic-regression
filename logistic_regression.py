# -*- coding: utf-8 -*-
"""LOGISTIC REGRESSION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NdUHv94Necg-aCuyJZStNPuJMuyKt5cm

# **Data Exploration**
"""

import numpy as np
import pandas as pd
df1 = pd.read_csv('Titanic_train.csv')
df2 = pd.read_csv('Titanic_test.csv')
df1

df2

df3 = df1.drop(['PassengerId', 'Name','Ticket'], axis=1)
df3

df3.shape

df3.describe()

df4 = df2.drop(['PassengerId', 'Name','Ticket'], axis=1)
df4

df4.shape

df4.describe()

df3.isnull().sum()

print(df3['Age'].median())
print(df3['Embarked'].mode())
print(df3['Cabin'].mode())

df3['Age'].fillna(df3['Age'].median(), inplace=True)
df3['Embarked'].fillna(df3['Embarked'].mode()[0], inplace=True)
df3['Cabin'].fillna(df3['Cabin'].mode()[0], inplace=True)

df3.isnull().sum()

df4.isnull().sum()

print(df4['Age'].median())
print(df4['Fare'].median())
print(df4['Cabin'].mode())

df4['Age'].fillna(df4['Age'].median(), inplace=True)
df4['Fare'].fillna(df4['Fare'].median(), inplace=True)
df4['Cabin'].fillna(df4['Cabin'].mode()[0], inplace=True)

df4.isnull().sum()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize = (20,17))
for i,column in enumerate(df3.columns):
    plt.subplot(3,4,i+1)
    sns.histplot(df3[column],kde = True)
    plt.title(f'Histogram of {column}')
plt.tight_layout()
plt.show()

plt.figure(figsize = (20,17))
for i,column in enumerate(df3.columns):
    plt.subplot(3,4,i+1)
    sns.boxplot(df3[column],vert=True)
    plt.title(f'Boxplot of {column}')

plt.tight_layout()
plt.show()

sns.pairplot(df3)
plt.show()

df4

plt.figure(figsize = (20,17))
for i,column in enumerate(df4.columns):
    plt.subplot(3,4,i+1)
    sns.histplot(df4[column],kde = True)
    plt.title(f'Histogram of {column}')
plt.tight_layout()
plt.show()

plt.figure(figsize = (20,17))
for i,column in enumerate(df4.columns):
    plt.subplot(3,4,i+1)
    sns.boxplot(df4[column],vert = True)
    plt.title(f'Boxplot of {column}')
plt.tight_layout()
plt.show()

sns.pairplot(df4)
plt.show()

"""# **Data Preprocessing**"""

# training data is converted into encoding
df3.info()

object_columns = df3.select_dtypes(include=['object'])
object_columns

integer = df3.select_dtypes(include=['int64','float64'])
integer

# data transformation
from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()
le = []
le = pd.DataFrame(le)
le['Sex'] = LE.fit_transform(object_columns['Sex'])
le['Cabin'] = LE.fit_transform(object_columns['Cabin'])
le['Embarked'] = LE.fit_transform(object_columns['Embarked'])
le

df5 = pd.concat([integer,le],axis=1)
df5.to_csv('Titanic_data.csv',index=True)

df4.info()

df4_cat= df4.select_dtypes(include=['object'])
df4_cat

from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()
dfle = []
dfle = pd.DataFrame(dfle)
dfle['Sex'] = LE.fit_transform(df4_cat['Sex'])
dfle['Cabin'] = LE.fit_transform(df4_cat['Cabin'])
dfle['Embarked'] = LE.fit_transform(df4_cat['Embarked'])
dfle

df4_num= df4.select_dtypes(include=['int','float'])
df4_num

df6 = pd.concat([df4_num,dfle],axis=1)
df6

"""# **Model Building**"""

Y = df5['Survived']
X = df5.iloc[:,1:8]

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X,Y)
Y_pred = model.predict(X)

from sklearn.metrics import confusion_matrix, accuracy_score
confusion_matrix(Y,Y_pred)

# Accuracy score

ac1 =  accuracy_score(Y,Y_pred)
print("accuracy_score :",np.round(ac1,3))

"""# **Model Evaluation**"""

# accuracy, precision, recall, F1-score

from sklearn.metrics import recall_score,precision_score,f1_score
rs = recall_score(Y,Y_pred)
print("Sencitivity score :",np.round(rs,3))
ps = precision_score(Y,Y_pred)
print("Precision score :",np.round(ps,3))
F1 = f1_score(Y,Y_pred)
print("F1 score :",np.round(F1,3))

df5['predict_proba']=model.predict_proba(X)[:,1]
df5['predict_proba']

from sklearn.metrics import roc_curve,roc_auc_score
fpr,tpr,dummy = roc_curve(Y,df5['predict_proba'])
plt.scatter(fpr,tpr)
plt.plot(fpr,tpr,color= 'r')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title("ROC")
plt.show()

"""# **Interpretation**"""

auc_score = roc_auc_score(Y,df5['predict_proba'])
print('Area Under Curve :',np.round(auc_score,3))

auc_score = roc_auc_score(Y,df5['predict_proba'])
print('Area Under Curve :',np.round(auc_score,3))

#b. Discuss the significance of features in predicting the target variable (survival probability in this case).

'''
# Feature Significance Discussion

1. Sex:  Male passengers had a lower survival rate than female passengers. This is a highly significant predictor.

2. Pclass: Passengers in higher classes (1st and 2nd) had better survival odds than those in 3rd class.
          Passenger class strongly correlates with survival.

3. Age: Age might show a complex relationship with survival.
        Children and young adults might have had a higher survival rate due to prioritization in lifeboats.  This needs further analysis beyond basic correlation.

4. Fare: A higher fare generally indicates a better cabin and potentially a higher social class,
        possibly correlating with increased survival probability.  Needs deeper exploration with visualizations.

5. SibSp (Siblings/Spouses):  The number of siblings or spouses aboard could affect survival.
          Large families might have faced challenges in getting on lifeboats together.
          A more nuanced analysis is required to interpret the impact.

6. Cabin: The presence or absence of cabin information ('cabin' feature) may indicate passenger class or location on the ship,
            impacting survival chances.  Passengers with registered cabins might have been located in more easily accessible areas during the evacuation.

7. Embarked: The port of embarkation might indirectly relate to passenger class or ticket type and could have influenced survival,
             but this is likely less significant than other factors.
'''

import pandas as pd
dff = pd.read_csv('Titanic_data.csv')
dff

dff['Embarked'].value_counts()

"""# **Deployment with Streamlit**"""

import pandas as pd
import streamlit as st
from sklearn.linear_model import LogisticRegression
st.title('Model Deployment : LogisticRegression')
st.sidebar.header('User Input Parameters')
def user_input_features():
    # # Input variables from the user

    Pclass = st.sidebar.selectbox("Passenger_class",('1','2','3'))
    Age = st.sidebar.number_input('Insert the age')
    SibSp = st.sidebar.selectbox("Number of Siblings/Spouses (SibSp)",('0','1','2','3','4','5','8'))
    Parch = st.sidebar.selectbox("Number of Parents/Children (Parch)",('0','1','2','3','4','5','6'))
    Fare = st.sidebar.number_input(" Insert the Fare value")
    Sex = st.sidebar.selectbox("Gender",('0','1'))
    Cabin = st.sidebar.number_input('Insert the cabin number ')
    Embarked = st.sidebar.selectbox('Embarked',('0','1','2'))
    # Combine inputs into a single array
    data = {'Pclass':Pclass,
            'Age':Age,
            'SibSp':SibSp,
            'Parch':Parch,
            'Fare':Fare,
            'Sex':Sex,
            'Cabin':Cabin,
            'Embarked':Embarked}
    features = pd.DataFrame(data,index = [0])
    return features
input_data = user_input_features()
st.subheader('User Input Parameters')
st.write(input_data)
Titanic_data = pd.read_csv('Titanic_data.csv')

# Create x and y before dropping 'Survived'
x = Titanic_data.iloc[:,2:]
y = Titanic_data['Survived'] # Now y contains the 'Survived' column

# Now you can safely drop 'Survived' from Titanic_data for prediction
Titanic_data.drop(['Survived'],inplace = True, axis = 1)
Titanic_data = Titanic_data.dropna()

# Make prediction
clf = LogisticRegression()
clf.fit(x,y)
prediction = clf.predict(input_data)
prediction_proba = clf.predict_proba(input_data)
st.subheader("Predicted Result") # Corrected typo: subhearder to subheader
st.write( "Survived" if prediction[0] == 1 else "Did Not Survive")
st.subheader('Prediction Probability')
st.write(prediction_proba)

"""# **What is the difference between precision and recall?**

Precision focuses on the accuracy of positive predictions, asking how many of the predicted positive cases were actually positive.

 Recall, on the other hand, focuses on capturing all actual positive cases, asking how many of all the actual positive cases were correctly identified

# **What is cross-validation, and why is it important in binary classification?**

*   Cross-validation is a technique used to assess the performance of a machine learning model, particularly important in binary classification, by splitting the data into multiple subsets (folds) and using some folds for training and others for testing, repeating this process to get a more reliable performance estimate
*   Cross-validation is crucial in binary classification to assess how well a model generalizes to unseen data and to prevent overfitting
"""